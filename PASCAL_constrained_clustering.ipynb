{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035c55ef",
   "metadata": {},
   "source": [
    "Rough outline on how to proceed with the poster, mostly my thinking out loud and collating relevant info on each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae7ca8-0dbb-4b91-9db1-8e8464702455",
   "metadata": {},
   "source": [
    "# Importing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bef49-a2d9-445d-8a8b-5d1eeb7225bf",
   "metadata": {},
   "source": [
    "Link to the Pascal challenge: \\\n",
    "http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html\n",
    "\n",
    "\n",
    "As per Sayantan's pdf, we are only going to be doing this part of the challenge:\\\n",
    "Classification: For each of the twenty classes, predicting presence/absence of an example of that class in the test image.\n",
    "\n",
    "In other words, no bounding boxes for individual features. We could optionally do that later but maybe now we can dispense with it, our task is to idenfity object classes and classify each image as containing a given class (out of 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944acbc-9393-47b5-a157-9454f860684e",
   "metadata": {},
   "source": [
    "# Vectorising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd899f-b1b1-4281-b57e-cc813e8bf941",
   "metadata": {},
   "source": [
    "Reference features provided in the task document are as follows:\n",
    "\n",
    "1. MPEG-7 Color Layout Descriptor \\\n",
    " https://en.wikipedia.org/wiki/Color_layout_descriptor\n",
    "2. Visual Bag-of-Words \\\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\n",
    "3. Speeded up robust features (SURF) \\\n",
    " https://en.wikipedia.org/wiki/Speeded_up_robust_features\n",
    "\n",
    "Again we could experiment with using different ones. We have to make sure we do feature extraction on a portion of the data, rather than the whole data, for CV, as Sayantan said this would be cheating \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6a7b4-997f-4c3e-b711-f6e7672fdcb6",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd411d-4fde-47d4-8b80-03f7f7805556",
   "metadata": {},
   "source": [
    "This is super important, one of the tasks being \"Extract meaningful hand-crafted features \\[...] using appropriate libraries or implement them from scratch. Reference to some hand-crafted features are provided. Identify and extract at least one extra feature of your choice other than the three features mentioned in the task. Why did you select this these feature(s) out of other options?\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da013f33-1eee-49d7-941b-40b777ede396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can locally install the open CV for the SURF alg\n",
    "# !pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a19c48-ca8b-49ab-aada-a23df52735bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread(\"./Data_train/JPEGImages/000005.jpg\")\n",
    "\n",
    "# cv.imshow(\"new window\", mat=img)\n",
    "# k = cv.waitKey(0) # Wait for a keystroke in the window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1f93a-86ff-489d-893a-86b3d8989dd1",
   "metadata": {},
   "source": [
    "# Algorithm selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870cb88-8775-4fd9-b5a6-7b89935b3405",
   "metadata": {},
   "source": [
    "I'm guessing we could get away with just picking one. I suggest constrained clustering, COP-k-Means. We could even experiment with running two separate algs and compare, like got with PCK-Means and compare the performance to COP-k-Means, they're quite similar\n",
    "\n",
    "Here is the link to the lecture slides about these algorithms.\\\n",
    "https://elearning.ovgu.de/pluginfile.php/937020/mod_resource/content/2/english_ATiML05_ConstrainedClustering.pdf\n",
    "\n",
    "Constrained clustering incorporates domain knowledge into the clustering process. In our case, domain knowledge would be super wide, essentially which features should be present for which object class. Domain knowledge that we can introduce includes \n",
    "1. Number of clusters (it's 20, Pascal tells us) \n",
    "2. Minimum cluster variance (not sure)\n",
    "3. Min / Max cluster size\n",
    "4. Must-link constraints\n",
    "5. Cannot-link constraints\n",
    "\n",
    "COP-k-Means is a modified k-means clustering algorithm that allows for constraints. It modifies step 1 of the k-means process (initialising cluster centres) by disallowing assignment to a cluster if the must constraint is already satisfied with another cluster or if the cannot-link constraint applies. \n",
    "\n",
    "The paper that describes the algorithm is here:\n",
    "https://web.cse.msu.edu/~cse802/notes/ConstrainedKmeans.pdf \\\n",
    "It's also where the slides got their low quality screenshot of algorithm steps from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296870a-53ea-4ba4-9a59-4b630fdec05f",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d69244-7647-4e38-a8c9-eab9015a099d",
   "metadata": {},
   "source": [
    "CV and the like, gotta plot that error as a function of lambda / M for Sayantan for sure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5b01c-eaf2-4c9b-ad7b-fd1805a1d5fe",
   "metadata": {},
   "source": [
    "# Generalisation error / testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3e711-b637-4a76-bbbe-10885d57831d",
   "metadata": {},
   "source": [
    "50 % of the Pascal dataset is set to be for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd31505",
   "metadata": {},
   "source": [
    "# Load and divide the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbff4df-8d71-43eb-b969-3f9a2a13d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split succeded\n",
      "Train size: 2505\n",
      "Test Size: 2506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"Classes: person, bird, cat, cow, dog, horse, sheep, aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, tv/monitor\"\"\"\n",
    "\n",
    "jpegPath = './Data_train/JPEGImages'\n",
    "annotationPath = \"./Data_train/Annotations\"\n",
    "images = []\n",
    "labels = []\n",
    "x_train, x_test, y_train, y_test\n",
    "\n",
    "def getImageLables(path: str) -> list[str]:\n",
    "    tempLabels = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "        bs_data = BeautifulSoup(data, \"xml\")\n",
    "\n",
    "        #GET IMAGE LABELS\n",
    "        foundObjects = bs_data.find_all('object') #This holds the labels for the image\n",
    "        for object in foundObjects:\n",
    "            labelWithTags = str(object.find('name'))\n",
    "            tempLabels.append(labelWithTags.removeprefix('<name>').removesuffix('</name>'))\n",
    "        f.close()\n",
    "    labels.append(tempLabels)\n",
    "\n",
    "def getImageData(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "        bs_data = BeautifulSoup(data, \"xml\")\n",
    "\n",
    "        #GET IMAGE LABELS\n",
    "        foundObjects = bs_data.find('filename') #This holds the labels for the image\n",
    "        imagePath = os.path.join(jpegPath, str(foundObjects).removeprefix('<filename>').removesuffix('</filename>'))\n",
    "        images.append(io.imread(imagePath)) #Load image from specified path (ndarray of color values)              \n",
    "\n",
    "def getTestTrainingData():\n",
    "    for annoXML in os.listdir(annotationPath):\n",
    "        path = os.path.join(annotationPath, annoXML)\n",
    "        getImageLables(path)\n",
    "        getImageData(path)\n",
    "    \n",
    "    global x_train, x_test, y_train, y_test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.5)\n",
    "\n",
    "getTestTrainingData()\n",
    "print(\"Train test split succeded\\nTrain size: \"+ str(len(x_train))+\"\\nTest Size: \"+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd2aeb-3df8-4a76-abb9-50370e3fdf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
